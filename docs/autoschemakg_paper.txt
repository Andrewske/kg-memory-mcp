AutoSchemaKG: Autonomous Knowledge Graph Construction through
Dynamic Schema Induction from Web-Scale Corpora

arXiv:2505.23628v1 [cs.CL] 29 May 2025

Jiaxin Bai* 1 , Wei Fan* 1 , Qi Hu* 1 , Qing Zong1 , Chunyang Li1 , Hong Ting Tsang1
Hongyu Luo1 , Yauwai Yim1 , Haoyu Huang2 , Xiao Zhou1 , Feng Qin1 , Tianshi Zheng1
Xi Peng3 , Xin Yao3 , Huiwen Yang3 , Leijie Wu3 , Yi Ji3
Gong Zhang3 , Renhai Chen3 , and Yangqiu Song1
1
CSE, HKUST 2 CSE, CUHK 3 Theory Lab, Huawei
{jbai, wfanag, qhuaf, qzong, cliei, httsangaj, hluoay, ywyimaa, tzhengad
xzhoucs, fqinac}@cse.ust.hk, haoyuhuang@link.cuhk.edu.hk
{pancy.pengxi, yao.xin1, huiwen.yang, leijie.wu1, jiyi13,
nicholas.zhang, chenrenhai}@huawei.com yqsong@cse.ust.hk
Abstract

et al., 2024) and complex reasoning tasks (Li et al.,
2024b). Yet despite their critical importance, current KG construction approaches remain hampered
by an inherent paradox: they require predefined
schemas created by domain experts, which fundamentally limits their scalability, adaptability, and
domain coverage.
We present AutoSchemaKG, a paradigmshifting framework that breaks this dependency by
enabling autonomous knowledge graph construction without predefined schemas (Ye et al., 2023).
Our approach leverages the semantic understanding
capabilities of large language models to simultaneously extract knowledge triples and dynamically
induce schemas directly from text, eliminating this
manual bottleneck in KG development (Zhang and
Soh, 2024; Li et al., 2024a; Wang et al., 2025).
Our framework distinguishes itself by modeling events alongside entities (Zhang et al., 2020,
2022), recognizing that real-world knowledge is dynamic rather than static (Tan et al., 2024). By treating events as first-class citizens, AutoSchemaKG
captures temporal relationships, causality, and procedural knowledge missed by entity-only graphs.
Experiments confirm that with event information
preserving over 90% of original passage content
versus just 70% for entity information alone.
Central to our innovation is the conceptualization (Wang et al., 2023c; Bai et al., 2024; Wang
et al., 2023b; He et al., 2024; Wang et al., 2024a,c)
process that drives schema induction. Rather than
simply extracting triples, we employ a sophisticated abstraction mechanism that generalizes specific entities, events, and relations into broader conceptual categories. This conceptualization serves
multiple critical functions: it creates semantic
bridges between seemingly disparate information,

We present AutoSchemaKG, a framework for
fully autonomous knowledge graph construction that eliminates the need for predefined
schemas. Our system leverages large language
models to simultaneously extract knowledge
triples and induce comprehensive schemas directly from text, modeling both entities and
events while employing conceptualization to organize instances into semantic categories. Processing over 50 million documents, we construct ATLAS (Automated Triple Linking And
Schema induction), a family of knowledge
graphs with 900+ million nodes and 5.9 billion edges. This approach outperforms stateof-the-art baselines on multi-hop QA tasks and
enhances LLM factuality. Notably, our schema
induction achieves 95% semantic alignment
with human-crafted schemas with zero manual
intervention, demonstrating that billion-scale
knowledge graphs with dynamically induced
schemas can effectively complement parametric knowledge in large language models1 .

1

Introduction

In the current era of information abundance, transforming vast amounts of unstructured data into
structured, machine-readable knowledge remains
one of the most significant challenges in artificial intelligence. Knowledge Graphs (KGs) have
emerged as the cornerstone technology for this
transformation (Zhao et al., 2024), providing the
semantic backbone for applications ranging from
search engines and question answering (Wu et al.,
2024; Chen et al., 2024b; Zong et al., 2024; Sun
et al., 2024b) to recommendation systems (Lyu
*

Equal contribution.
https://github.com/HKUST-KnowComp/
AutoSchemaKG
1

1

Question: Who won the Indy Car Race in the largest populated city of the state where the performer of
Mingus Three is from?
[Concept]

United State

State

[Concept]

Arizona

[is the capital]

Myth
[Concept]

[Concept]

Champion
[Concept]

Mario Andretti

Country

[Participated]

race

[Concept]

[Concept]
The final
career victory
for Indy legend
Mario Andretti
(1993)

City

[At the same time]

Phoenix

[Participated]
Phoenix has a
rich history of
open wheel races

Figure 1: This figure demonstrates the structure of ATLAS, which stands for Automated Triple Linking And Schema
induction. Entity nodes (blue) and event (green) nodes are extracted from text, and Concept nodes (orange) are
obtained by schema induction.

enables zero-shot inferencing across domains, reduces sparsity in KGs, and provides hierarchical
organization that supports both specific and abstract reasoning (Wang et al., 2024d,b). Our approach transforms the traditional static schema into
a dynamic, multi-level conceptual framework that
adapts to new domains without predefined ontologies. This conceptualization layer represents a fundamental advancement, transforming a collection
of disconnected facts into a coherent knowledge
ecosystem with emergent reasoning capabilities.
By applying our framework to the Dolma 1.7
(Soldaini et al., 2024) pretraining corpus across
three diverse subsets, English Wikipedia, paper abstracts from Semantic Scholar, and 3% of Common
Crawl data, we construct the ATLAS family of
knowledge graphs (ATLAS-Wiki, ATLAS-Pes2o,
and ATLAS-CC). Collectively, these knowledge
graphs comprise over 900 million nodes connected
by 5.9 billion edges, containing billions of facts that
are comparable in scale to the parametric knowledge stored in large language models. This scale is
crucial, as we demonstrate that knowledge graphs
must reach a critical mass of billions of facts to
effectively compete with and complement the parametric knowledge in contemporary LLMs.
Our work addresses a fundamental research
question: can retrieval-augmented generation with
comparable-sized knowledge graphs enhance LLM
performance even when the retrieval corpus is
drawn from the same sources as the model’s pretraining data? Through comprehensive evaluations on general benchmarks, we demonstrate that
properly structured knowledge representations offer advantages over traditional text-based retrieval
even in these scenarios. Most significantly, our
schema induction process achieves 95% semantic

alignment with human-crafted schemas with zero
manual intervention, demonstrating that automated
schema generation can match expert quality while
dramatically improving construction efficiency.
The power of AutoSchemaKG extends beyond
its construction methodology to deliver substantial performance improvements in downstream applications. In rigorous evaluations, our approach
outperforms state-of-the-art baselines by 12-18%
on multi-hop question answering tasks (Trivedi
et al., 2022; Yang et al., 2018; Ho et al., 2020)
and enhances large language model factuality by
up to 9% (Chen et al., 2023). Moreover, we found
that our constructed knowledge graph is helpful
for Llama3.1 7B models on general reasoning task
on various domains that intensively requeries background knowledge, including Global Facts, History,
Law, Religion, Philosophy and Ethics, Medicine
and Health, and Social Sciences. These gains stem
from our system’s ability to create richer semantic
representations through the integration of entities,
events, and their conceptual abstractions—enabling
better reasoning over complex information across
different domains and data sources. Our key contributions include:
(1) Autonomous Schema Induction: We introduce a new approach that generates comprehensive
schemas directly from text, eliminating the need for
predefined ontologies while maintaining semantic
coherence and domain coverage.
(2) Unified Triple Extraction: We develop an
entity-event-concept extraction framework that captures not only traditional entity relationships but
also complex event structures and their conceptual categorizations, creating a multi-dimensional
knowledge representation.
(3) Scale-Optimized Architecture: We present
2

techniques for efficient knowledge extraction and
integration at web scale, processing billions of
triples while maintaining semantic consistency and
supporting real-time updates.
(4) Cross-Domain Adaptability: We demonstrate AutoSchemaKG’s effectiveness across diverse domains without domain-specific customization, offering a truly general-purpose knowledge
acquisition framework.
AutoSchemaKG represents a fundamental rethinking of knowledge graph construction, transforming what was once a heavily supervised process requiring significant domain expertise into a
fully automated pipeline. This advancement not
only accelerates KG development but also dramatically expands the potential application domains for
knowledge-intensive AI systems.

texts by filtering for English language and segmenting documents that exceed token limits. The segmented texts are grouped into processing batches.
Stage 1 extracts Entity-Entity relationships using
a system prompt PEE that instructs the LLM to
detect entities and their interrelations. The output
is parsed into triples (e1 , r, e2 ), where e1 , e2 ∈ VN
are entity nodes and r ∈ R is a relation type. Stage
2 identifies Entity-Event relationships with prompt
PEV , producing triples (e, r, v) or (v, r, e), where
e ∈ VN , v ∈ VE , and r ∈ R. Stage 3 targets
Event-Event relationships with prompt PV V , generating triples (v1 , r, v2 ), where v1 , v2 ∈ VE and
r ∈ R. The pipeline supports various LLMs with
optimized precision settings and GPU acceleration.
Extracted triples with their corresponding texts and
metadata are serialized into JSON files.

2

3.2

Problem Definition

Following triple extraction, we perform schema
induction to abstract specific entities, events, and
relations into generalized types. This process uses
LLMs to generate conceptual phrases representing
types of each graph element, aligning with our formal definition G = (V, E, C, ϕ, ψ). For each category (events, entities, and relations), we process
elements in batches. The LLM generates at least
three phrases per element that encapsulate its type
or related concepts at varying abstraction levels.
For entities (e ∈ VN ), we enhance abstraction by
incorporating contextual information from neighboring nodes. We sample up to Nctx neighbors to
construct a context string that provides additional
semantic cues. The schema induction pipeline processes the graph serialized from the triple extraction phase. Elements are partitioned into batches,
with options for slicing for distributed computation.
The generated phrases are recorded in a CSV file,
mapping each node v ∈ V and relation r ∈ R to
a subset of concepts in C via ϕ and ψ. This automated schema enhances the knowledge graph’s
adaptability across varied domains without requiring manual curation.

We formally outline the tasks involved in automatically constructing knowledge graphs. We begin
by providing a precise definition of a knowledge
graph equipped with a conceptual schema.
Definition 1 (Knowledge Graph with Conceptual
Schema). Consider a knowledge graph denoted
as G = (V, E, C, ϕ, ψ), where: V = VE ∪ VN
represents the collection of nodes, with VE as the
set of event nodes, VN as the set of entity nodes,
and VE ∩ VN = ∅. E ⊆ V × V × R defines the set
of edges, where R denotes relation types. Edges
may connect entity-entity, entity-event, or eventevent nodes. C is the set of conceptual categories.
ϕ : V → P(C) assigns each node a subset of
concepts, where ϕ(v) ⊆ C for every v ∈ V . ψ :
R → P(C) links each relation type to a subset
of concepts, where ψ(r) ⊆ C for every r ∈ R.
P(C) denotes the power set of C, encompassing
all possible subsets. Additional constraints: ∀v ∈
V : ϕ(v) ̸= ∅ and ∀r ∈ R : ψ(r) ̸= ∅.

3

AutoSchemaKG Framework

In this section, we elaborate on the process of fully
automating knowledge graph construction.

4
3.1

Schema Induction

Construction of ATLAS Families

Triple Extraction
Corpora As shown in Table 1, the ATLAS-Wiki,
ATLAS-Pes2o, and ATLAS-CC are constructed
from the subsets from Dolma’s subset of Wikipedia
& Wikibooks, Semantic Scholar, and Dolma’s CC
respectively.2 We use the full Wikipedia & Wiki-

Our approach employs a multi-phase pipeline using
Large Language Models to convert unstructured
text into knowledge triples from the Dolma corpus
(Soldaini et al., 2024). This pipeline extracts EntityEntity, Entity-Event, and Event-Event relationships
through three sequential stages. We preprocess

2

3

https://huggingface.co/datasets/allenai/dolma

Question Answering Corpora

# Text Chunks
# Entities
# Events
# Concepts
# Nodes
# Entity-Entity Edges
# Event-Entity Edges
# Event-Event Edges
# Conceptulization Edges
# Edges

Pre-training Corpora

MuSiQue

2WikiQA

HotpotQA

ATLAS-Wiki

ATLAS-Pes2o

ATLAS-CC

11,656
108,582
99,747
37,414
245,743

6,119
48,782
50,910
19,830
119,522

9,221
95,686
82,833
32,410
210,929

9.599M
70.104M
165.717M
8.091M
243.912M

7.918M
75.857M
92.636M
5.895M
174.387M

35.040M
241.061M
696.195M
31.070M
937.256M

91,186
143,254
45,157
933,330
1,212,927

40,748
63,680
21,062
432,869
558,359

78,467
123,527
36,602
789,608
1,028,204

0.114B
0.265B
0.071B
1.041B
1.492B

0.076B
0.208B
0.044B
0.821B
1.150B

0.414B
1.063B
0.295B
4.178B
5.958B

Table 1: Statistics of knowledge graph construction across QA datasets (MuSiQue, 2WikiQA, HotpotQA) and
LLM pre-training corpora (En-Wiki, Pes2o-Abstract, Common Crawl) for ATLAS knowledge graphs, showing
counts of text chunks, nodes (entities/events), concepts, edges (Entity-Entity, Event-Entity, Event-Event), and
conceptualizations. M = million, B = billion.

books to construct the ATLAS-Wiki, and we use
the abstract part of Semantic Scholar to construct
ATLAS-Pes2o, and we use the each of 3% from cchead, cc-middle, and cc-tail to construct ATLASCC. According to (Soldaini et al., 2024) the head,
middle, tail of CC are used to measure the distribution similarity to Wikipedia text.
Computational Cost We constructed our
knowledge graphs using 80GB GPUs with
1,513 TFLOPS of FP16 compute, running
Llama-3-8B-instruct with Flash Attention. The
computational demands were substantial: 14,300
GPU hours for En-Wiki (243.9M nodes, 1.49B
edges), 11,800 GPU hours for Pes2o-Abstract
(174.4M nodes, 1.15B edges), and 52,300 GPU
hours for Common Crawl (937.3M nodes, 5.96B
edges). Processing 1024-token chunks in batches,
we invested approximately 78,400 GPU hours total
to extract billions of semantic relationships.

5

Triple Type

ATLAS-Wiki

Entity-Entity
Event-Entity
Event-Event

99.13
100.0
99.60

90.10
92.59
93.59

94.09
95.60
96.01

ATLAS-Pes2o

Entity-Entity
Event-Entity
Event-Event

97.66
100.0
99.54

89.89
94.29
91.31

93.03
96.83
94.94

ATLAS-CC

Entity-Entity
Event-Entity
Event-Event

95.65
99.93
99.86

84.64
87.92
93.20

88.82
92.72
96.16

Precision Recall

F1

Table 2: Triple precision, recall and F1 score across
datasets. Each row displays the performance of a type
of extracted triples.

DeepSeek-V3 lists facts present in the original text
but missing from the extracted triples (false negatives); This methodology allows us to calculate
precise metrics: (1) Precision: proportion of correctly extracted triples out of all extracted triples;
(2) Recall: proportion of correctly extracted triples
among all ground-truth triples in the text; (3) F1
score: the harmonic mean of precision and recall.
As shown in Table 2, our approach demonstrates
exceptional extraction quality across all datasets,
with particularly strong performance on Wikipedia
content. The precision, recall, and F1 scores of
the triples in our KG exceed 90% in most cases,
demonstrating the high quality and reliability of
our extracted triples.

Experiment

In the this section, we show that the AutoSchemaKG has accurate triplex extraction, can
coherently induce schemas, and has very high information preservations in section 5.1.
5.1

Knowledge Graph

Evaluating AutoSchemaKG

Evaluating Triple Extraction Accuracy We
use a rigorous counting-based evaluation method.
Rather than relying on subjective scoring, we employ DeepSeek-V3 (Liu et al., 2024) as a judge in
a structured verification process. For each document: (1) We present DeepSeek-V3 with both the
original text and the triples extracted by Llama3-8B-Instruct; (2) DeepSeek-V3 identifies triples
that are incorrectly extracted (false positives); (3)

Measuring Information Preservation in Knowledge Graphs We evaluate the effectiveness of
the entity-level triples and event-level triples of our
constructed KG in preserving information from
original passages. We test how well multiplechoice question (MCQ) performance is preserved
when we convert the original passage into KG data.
Following the evaluation protocol from the exist4

Dataset

Context

Model

Task

LLaMA-3-8B LLaMA-3-70B

Dataset

FB15kET
YAGO43kET
Event Typing
wikiHow
Relation Typing FB15kET

88.57
80.67
99.18
88.75

86.54
58.86
99.26
88.41

75.05-99.49
83.33
97.78
97.98

FB15kET
YAGO43kET
LLaMA-3-13B
Event Typing
wikiHow
Relation Typing FB15kET

89.25
94.26
98.97
88.58

88.59
90.56
99.33
88.66

70.25-99.10
81.01
96.78
96.98

FB15kET
YAGO43kET
LLaMA-3-70B
Event Typing
wikiHow
Relation Typing FB15kET

89.49
94.61
99.41
88.70

87.30
92.64
99.15
90.33

Entity Typing
[Lower-Upper]
Entity
Event
Event + Entity

46.29-99.29
65.08
92.69
93.30

65.69-99.70
70.96
94.82
95.13

[Lower-Upper]
Entity
ATLAS-Pes2o
Event
Event + Entity

62.32-98.99
80.00
96.97
97.37

[Lower-Upper]
Entity
Event
Event + Entity

56.08-97.29
76.78
94.87
96.28

ATLAS-Wiki

ATLAS-CC

BS-R BS-C

LLaMA-3-8B

Entity Typing

Entity Typing

Table 3: KG performance across datasets, showing
bounds (no context to full passage) and results with
different knowledge representations. Entity, Event, and
combined representations preserve most information for
MCQs, approaching full-passage performance across
all datasets and models.

Table 4: Results of schema induction with various
LLaMA family LLMs across three kinds of typing tasks.

details are in Appendix C.1. Since rule-based evaluation might overlook semantic similarities, we use
two semantic-level metrics: BS-R and BS-C, explained in Appendix C.2. Table 4 shows results using three sizes of LLaMA-3. Our method achieves
over 80% and often 90% recall for entity, event,
and relation types in most cases, with performance
improving as LLM parameter size increases.

ing work (Schuhmann et al., 2025), we generate
five MCQs with LLaMA-3-70B-Instruct for each
original passage, and the prompts are shown in
Figure 8. We sample 200 original passages and
1,000 MCQs are obtained for each dataset. We ask
LLMs to answer them with no context (denoted as
lower bound), then ask them again with the original
passage (denoted as upper bound) for sanity check.
Finally, we conduct tests using entity-level triples
(denoted as Entity), event-level triples (denoted
as Event), and a combination of both entity-level
and event-level triples (denoted as Event + Entity).
We evaluate on three pre-training datasets with
our constructed KG in Table 1: En-Wiki, Pes2oAbstract and Common Crawl. According to the
results shown in Table 3, we have the following
insights: (1) Information is well preserved in
our constructed KG. MCQs performance with Entity, Event or Event + Entity remains far above the
lower bound baseline and approaches the originalpassage upper bound. It suggests that the information in the original passages is well preserved in
our constructed KG; (2) Events are more effective
than entities. The MCQs performance with Event
or Event + Entity is much closer to the upper bound
than that with Entity, which accuracy is more than
95% in most of the cases. It demonstrates that the
event-level triples can preserve richer and more
precious information than entity-level triples.

5.2

Performance on Multi-hop QA Tasks

This subsection details the experimental setup for
open-domain QA, focusing on multi-hop reasoning
tasks where our knowledge graph’s structure and
schema induction are expected to excel.
Datasets We select three benchmark datasets
known for their multi-hop reasoning demands, all
derived from Wikipedia: MuSiQue (Trivedi et al.,
2022), HotpotQA (Yang et al., 2018), and 2WikiMultihopQA (Ho et al., 2020), necessitating complex relational paths across articles. From each
dataset, we randomly select one thousand questions
following (Gutiérrez et al., 2024).
Baselines and Metrics We compare our knowledge graph-based RAG system against several
state-of-the-art approaches. The graph-based baselines include HippoRAG (Gutiérrez et al., 2024), a
framework that builds a memory graph from text using entity recognition and relation extraction; HippoRAG2 (Gutiérrez et al., 2025), an advanced iteration with enhanced graph construction; GraphRAG
(Edge et al., 2024), Microsoft Research’s technique
combining text extraction, network analysis, and
LLM prompting; LightRAG (Guo et al., 2024), a
simpler alternative to GraphRAG focused on efficiency; and MiniRAG (Fan et al., 2025), an ex-

Measuring Schema Quality To demonstrate our
schema induction method’s capability, we apply it
to entity, event, and relation typing tasks, measuring how many types our method recalls. Dataset
5

Model/Dataset

MuSiQue

Metric

EM

F1

EM

2Wiki
F1

EM

F1

17.6
24.0
20.3

26.1
31.3
28.8

36.5
38.1
47.9

42.8
41.9
51.2

37.0
51.3
52.0

47.3
62.3
63.4

30.6
33.6
34.7

40.9
44.8
45.7

55.1
55.8
57.5

60.0
60.6
61.5

58.6
60.7
62.8

71.0
73.3
75.3

47.3
51.4
38.6
13.2
65.0
65.0

52.1
58.6
44.6
21.4
71.8
71.0

56.8
55.2
33.3
47.1
52.6
62.7

69.5
68.6
44.9
59.9
63.5
75.5

Think on Graph Settings We implement Think
on Graph (ToG) (Sun et al., 2024a) using a knowledge graph derived from our corpus. Nodes
represent extracted entities and concepts, with
edges showing semantic relations.
We use
multi-qa-MiniLM-L6-dot-v1 to compute embeddings for all graph elements, indexed with FAISS.
LLama-3.3-70B-Instruct performs entity recognition, path scoring, reasoning, and answer generation. The workflow extracts query entities, retrieves
starting nodes, explores graph paths through depthlimited search, prunes irrelevant paths, and generates answers based on retrieved paths. Algorithm 1
in the Appendix provides details.

HotpotQA

Baseline Retrievers
No Retriever
Contriever
BM25
LLM Embeddings
GTE-Qwen2-7B
GritLM-7B
NV-Embed-v2 (7B)

Existing Graph-based RAG Methods
RAPTOR
GraphRAG
LightRAG
MiniRAG
HippoRAG
HippoRAG2

20.7
27.3
20.0
9.6
26.2
37.2

28.9
38.5
29.3
16.8
35.1
48.6

AutoSchemaKG (LLama-3.1-8B-Instruct) + Think-on-Graph
Entity-KG
Entity-Event-KG
Full-KG

14.8
19.4
20.1

26.0
32.8
31.2

36.9
39.0
40.0

44.0
47.1
47.7

41.9
47.7
48.2

HippoRAG 1&2 Settings In our implementation
of HippoRAG (Gutiérrez et al., 2024), we extend
the original framework to operate on a customized
graph. Initially presented in the foundational paper,
we employ Named Entity Recognition (NER) to
build a personalized dictionary for PageRank execution. Regarding HippoRAG2 (Gutiérrez et al.,
2025), we select the top 30 edges (musique dataset
50 edges) for LLM filtering, incorporating a weight
adjustment factor of 0.9. Considering the capability of our graph to effectively locating subgraphs,
combined with various graph configurations (entity, event, concept) resulting in graphs of differing
densities, we set the damping factor to 0.9 to concentrate on propagation within the local subgraph.
For further implementation details, please refer to
Algorithm 2.

55.2
61.2
60.5

AutoSchemaKG (LLama-3.1-8B-Instruct) + HippoRAG
Entity-KG
Entity-Event-KG
Full-KG

22.5
22.9
23.6

36.4
36.1
36.5

57.7
56.4
54.8

65.8
64.4
63.2

50.3
48.6
50.0

65.8
64.6
65.3

AutoSchemaKG (LLama-3.1-8B-Instruct) + HippoRAG2
Entity-KG
Entity-Event-KG
Full-KG

31.4
31.6
31.8

47.2
47.3
47.3

64.2
65.2
65.3

73.3
73.7
73.9

60.9
60.0
61.8

77.5
77.0
78.3

Table 5: Performance comparison of AutoSchemaKG
integrated with ToG, HippoRAG and HippoRAG2 with
bold indicating the highest performance per dataset.

tremely simple framework tailored for Small Language Models. For MiniRAG and LightRAG we
adjust its prompts so that it outputs brief answers
for QA instead of generating long answers with
explanations. For text-based RAG comparisons,
we evaluate against BM25 + LLM using traditional
retrieval with BM25 scoring; Contriever (Izacard
et al., 2021), a dense retrieval-augmented system
fine-tuned for QA; and RAPTOR (Sarthi et al.,
2024), a hierarchical summarization system. These
baselines allow us to benchmark our approach
against diverse retrieval-augmented methods.

Implementation Details The knowledge graph
is constructed from the corresponding context corpora for each dataset folowing (Gutiérrez et al.,
2024) using the framework of AutoSchemaKG
with Lmax = 1024 and B = 16, and the schema
induction pipeline (Section B.2) with Bs = 5. We
employ Meta’s LLaMA-3.1-8B-Instruct to construct the graphs, optimized with bfloat16 precision and Flash Attention 2. The graph is stored
in NetworkX for retrieval, with subgraphs fed into
LLaMA-3.3-70B-Instruct for answer generation.

We evaluate our system using standard metrics for open-domain QA. Exact Match (EM)
measures binary correctness after normalization:
EM(a, g) = 1[norm(a) = norm(g)], where a is
the predicted answer, g is the gold answer, and
normalization includes lowercasing and removing
articles, punctuation, and whitespace. F1 Score
measures token overlap between normalized an·R
swers: F1 = 2·P
P +R , where P = |a ∩ g|/|a| and
R = |a ∩ g|/|g| are precision and recall.

Evaluation Results The experimental results
in Figure 5 and Figure 8 demonstrate AutoSchemaKG’s effectiveness in multi-hop question answering across three benchmark datasets.
With HippoRAG2 integration, the Full-KG configuration (entities, events, and concepts) outperforms traditional retrieval approaches like
BM25 and Contriever by 12-18%, highlight6

Corpus

Method

Acc

F1

-

-

54.08

26.79

Wikipedia

Random
BM25
Dense Retrieval

52.77
56.15
56.04

25.56
30.43
30.33

Pes2o-Abstract

Random
BM25
Dense Retrieval

53.34
54.60
55.43

26.00
27.95
29.19

Common Crawl

Random
BM25
Dense Retrieval

53.31
54.56
54.42

26.45
28.32
28.49

Freebase

Think on Graph

53.75

24.81

ATLAS-Wiki
ATLAS-Pes2o
ATLAS-CC

HippoRAG2

56.43
55.30
55.56

30.48
28.12
29.57

ing vanilla settings for math and reasoning domains.
For a comprehensive comparison, we evaluated
against multiple retrieval methods: HippoRAG v2,
BM25, and dense retrieval using MiniLM (Wang
et al., 2021). The retrieval process on text corpora is implemented in ElasticSearch database system (Elasticsearch, 2018). Our decision to use
MiniLM rather than larger language model-based
embedding approaches was driven by computational constraints. Implementing dense retrieval
with higher-dimensional embeddings (e.g., 4096
dimensions) across our one billion nodes would require approximately 16 terabytes of storage using
standard 32-bit floating-point representation. These
baselines represent state-of-the-art approaches in
graph-based RAG and standalone retrieval systems.
All experiments were implemented using the same
LLaMA-3.1-8B-Instruct model with Neo4j integration and zero-shot CoT settings, ensuring fair
comparison across methods. Performance was
measured using balanced accuracy (giving equal
weight to true and false segments) and F1 score
for detecting factual errors (Table 6). Our results
demonstrate that HippoRAG2 with our KG consistently outperforms baselines on Wikipedia (56.43%
accuracy, 30.48% F1) and Common Crawl corpora, while achieving competitive results on Pes2oAbstract. The superior performance on Wikipedia
likely stems from FELM samples being partially
sourced from Wikipedia content. Detailed implementation specifics and extended results are available in Appendix G.1.

Text Corpora

Knowledge Graph

Table 6: Balanced accuracy (%) and F1 score (%) on
FELM benchmark of Llama-3.1-8b-Instruct with
retrieval methods. The best results are in bold, and the
second best results are underlined.

ing its strength in complex reasoning scenarios.
Notably, AutoSchemaKG achieves comparable
or better results using LLaMA-3.1-8B-Instruct
as graph constructor compared to the original HippoRAG2 implementation that requires
LLaMA-3.3-70B-Instruct for both construction
and QA reading.
Advantages of Events and Concepts Our case
studies revealed two key benefits of event and concept nodes: 1) Event nodes provide enriched context. As shown in Figure 9, they serve as valuable
retrieval targets when critical information in triples
is ambiguous or missed, helping identify relevant
subgraphs containing passage nodes; 2) Concept
nodes create alternative pathways. These nodes
establish connections beyond direct entities and
events, addressing complex multi-hop question answering limitations. Figure 10 shows how concept
nodes link knowledge across disparate subgraphs,
enabling systems like HippoRAG to bridge separate subgraph influences via PageRank algorithms.
5.3

5.4

General Domain Knowledge Capabilities

To assess AutoSchemaKG’s ability to construct
knowledge graphs across various domains, we evaluated it on MMLU (Hendrycks et al., 2021), a comprehensive benchmark for LLM reasoning. Previous research on KNN-LMs (Khandelwal et al.)
suggests that retrieval-augmented generation can
sometimes hinder LLMs’ reasoning capabilities
(Wang et al., 2023a; Geng et al., 2025). While
we do not expect RAG to universally improve
LLM performance, our findings demonstrate significant improvements in knowledge-intensive domains, even those covered in LLM training data.
Using the same retrieval and generation settings as
our FELM experiments, we classified MMLU tasks
into subject categories (detailed mapping in Appendix G.2) and focused on knowledge-intensive
domains including History, Law, Religion, Philosophy/Ethics, Medicine/Health, Global Facts, and

Enhancing LLM Factuality with KGs

We evaluated our KG’s effectiveness in enhancing factuality using the FELM benchmark (Chen
et al., 2023), which contains 847 samples across
five domains with 4,425 fine-grained text segments.
Following FELM’s protocol, we applied RAG to
three domains (world knowledge, science/technology, and writing/recommendation) while maintain7

Knowledge Source

History

Law

Religion

Phil/Eth

Med/Hlth

GlbFct

SocSci

Average

None
Freebase-ToG

76.59
78.42

66.86
69.00

83.04
75.44

63.55
65.67

70.38
72.65

66.72
67.27

79.74
76.00

72.41
72.06

76.64
76.24
74.89

66.82
64.16
66.52

79.53
80.70
79.53

59.26
62.01
61.74

70.34
70.62
69.82

66.46
66.59
68.11

77.78
77.16
77.52

70.98
71.07
71.16

67.35
65.89
66.36

78.36
78.95
80.12

63.34
63.83
60.43

69.35
71.01
69.58

61.98
65.78
64.67

76.99
77.07
76.71

70.58
71.51
70.57

69.60
61.82
68.98

79.53
78.36
79.53

63.58
65.15
60.46

70.82
69.72
69.29

62.41
66.77
64.09

76.83
76.47
75.21

70.91
70.58
70.29

67.38
68.41
70.85

84.21
81.29
83.04

66.01
65.05
65.60

70.82
72.75
71.28

68.36
65.67
63.95

79.16
81.19
78.16

73.24
73.07
73.01

Random Baseline
Wikipedia
Pes2o-Abstract
Common Crawl

Text Corpora + BM25
Wikipedia
Pes2o-Abstract
Common Crawl

76.67
78.01
76.15

Text Corpora + Dense Retrieval
Wikipedia
Pes2o-Abstract
Common Crawl

73.59
75.79
74.47

ATLAS + HippoRAG2
ATLAS-Wiki
ATLAS-Pes2o
ATLAS-CC

76.73
77.13
78.16

Table 7: Performance comparison of Llama-3.1-8b-Instruct with our KG-integrated HippoRAG2 versus baseline
methods across Wikipedia, Common Crawl, and Pes2o-Abstract corpora on MMLU benchmarks. Tasks are grouped
by subject, with bold and underlined values indicating first and second-highest scores. Phil/Eth, Med/Hlth, GlbFct,
and SocSci denote Philosophy/Ethics, Medicine/Health, Global Facts, and Social Sciences.

Social Sciences.

6

As shown in Table 7, our ATLAS knowledge
graphs enhanced performance across these domains
on all tested corpora. Notably, each ATLAS variant demonstrated distinct strengths: ATLAS-Pes2o
excelled in Medicine/Health and Social Sciences,
reflecting its academic paper-sourced knowledge;
ATLAS-Wiki showed advantages in general knowledge areas like Religion, Philosophy/Ethics, and
Global Facts; while ATLAS-CC performed best
in Law and History, leveraging its broader websourced content. All ATLAS variants consistently
outperformed both the no-retrieval baseline and
Freebase-ToG in these humanities and social science domains. For example, in Law, our approach
achieved a 4-point improvement over the baseline,
while some other retrieval methods actually decreased performance, as shown in Table 12.

Knowledge graph (KG) construction transforms
unstructured data into machine-readable formats.
Traditional approaches using predefined schemas
limit cross-domain scalability, while LLMs now
enable autonomous construction through improved
extraction and schema induction. Recent advances
include SAC-KG (Chen et al., 2024a), which
uses LLMs as domain experts to generate specialized KGs with high precision on million-node
graphs; Docs2KG (Sun et al., 2024c) for heterogeneous document processing; and KAG (Liang
et al., 2024), which enhances multi-hop reasoning
through KG-text mutual indexing. Schema induction automatically derives KG structure without
predefined ontologies. Hofer et al. (2024) survey
construction pipelines emphasizing ontology learning, while Dash et al. (2021) address canonicalization using variational autoencoders and Dognin
et al. (2021) employ reinforcement learning for
bidirectional text-to-graph conversion.

The domain-specific performance pattern aligns
with intuitive expectations: knowledge graphs excel in retrieving factual relationships critical for
humanities and social sciences, while showing limited benefits in mathematical and technical domains
where node-relation structures are less effective for
capturing procedural knowledge. The complete
subject analysis, including technical domains, is
available in Appendix G.2.

7

Further Related Work

Conclusion

AutoSchemaKG transforms knowledge graph
construction by eliminating predefined schemas
through LLM-based triple extraction and schema
induction. Our methodology constructs the AT8

LAS family of knowledge graphs (900+ million
nodes, 5.9 billion edges) with high-quality triple
extraction (>95% precision) and schema induction.
This approach outperforms baselines on multi-hop
QA tasks and enhances LLM factuality across diverse domains. Our work demonstrates that billionscale knowledge graphs with dynamically induced
schemas can be effectively constructed without expert intervention, providing valuable complements
to parametric knowledge in large language models.

8

tially enabling more accurate and explainable AI
systems across diverse applications and domains.

References
Jiaxin Bai, Zhaobo Wang, Junfei Cheng, Dan Yu, Zerui
Huang, Weiqi Wang, Xin Liu, Chen Luo, Qi He, Yanming Zhu, Bo Li, and Yangqiu Song. 2024. Intention
knowledge graph construction for user intention relation modeling. Preprint, arXiv:2412.11500.
Kurt D. Bollacker, Colin Evans, Praveen K. Paritosh,
Tim Sturge, and Jamie Taylor. 2008. Freebase: a
collaboratively created graph database for structuring
human knowledge. In Proceedings of the ACM SIGMOD International Conference on Management of
Data, SIGMOD 2008, Vancouver, BC, Canada, June
10-12, 2008, pages 1247–1250. ACM.

Limitations

Despite promising results, our work has several
important limitations. The construction of billionscale knowledge graphs requires substantial computational resources (78,400+ GPU hours), limiting accessibility for researchers with resource constraints. Our approach inherits biases and limitations from the underlying LLMs used for triple
extraction and schema induction, potentially affecting performance in specialized domains where
these models lack expertise. While achieving high
semantic alignment with human-crafted schemas,
our induction method still struggles with extremely
technical domains requiring expert-level conceptual organization. Despite extracting billions of
facts, our knowledge graphs may contain inconsistencies, contradictions, or information gaps in
sparse knowledge regions.

9

Antoine Bordes, Nicolas Usunier, Alberto GarcíaDurán, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multirelational data. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake
Tahoe, Nevada, United States, pages 2787–2795.
Hanzhu Chen, Xu Shen, Qitan Lv, Jie Wang, Xiaoqi
Ni, and Jieping Ye. 2024a. SAC-KG: Exploiting
large language models as skilled automatic constructors for domain knowledge graph. In Proceedings
of the 62nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 4345–4360, Bangkok, Thailand. Association
for Computational Linguistics.

Ethical Statement

Muhao Chen, Hongming Zhang, Haoyu Wang, and
Dan Roth. 2020. " what are you trying to do?" semantic typing of event processes. arXiv preprint
arXiv:2010.06724.

Our research on AutoSchemaKG and ATLAS
knowledge graphs adheres to rigorous ethical standards. We exclusively utilized publicly available
corpora (Dolma 1.7) with proper attribution and
transparently disclosed our computational requirements (approximately 78,400 GPU hours). We acknowledge potential inherited biases from source
texts and the large language models used in our
pipeline, while emphasizing that our approach minimizes manual intervention that might introduce
additional biases. Privacy considerations were addressed by processing only public data without
extracting personally identifiable information. We
candidly discuss limitations including resource constraints, LLM biases, and challenges with specialized domains. Through detailed methodological
descriptions, we promote reproducibility and scientific validation. Our work aims to democratize
knowledge graph construction without requiring
specialized expertise or predefined schemas, poten-

Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern,
Siyang Gao, Pengfei Liu, and Junxian He. 2023.
Felm: benchmarking factuality evaluation of large
language models. In Proceedings of the 37th International Conference on Neural Information Processing
Systems, NIPS ’23, Red Hook, NY, USA. Curran
Associates Inc.
Zhongwu Chen, Long Bai, Zixuan Li, Zhen Huang, Xiaolong Jin, and Yong Dou. 2024b. A new pipeline
for knowledge graph reasoning enhanced by large
language models without fine-tuning. In Proceedings of the 2024 Conference on Empirical Methods
in Natural Language Processing, pages 1366–1381,
Miami, Florida, USA. Association for Computational
Linguistics.
Sarthak Dash, Gaetano Rossiello, Nandana Mihindukulasooriya, Sugato Bagchi, and Alfio Gliozzo. 2021.
Open knowledge graphs canonicalization using variational autoencoders. In Proceedings of the 2021

9

Conference on Empirical Methods in Natural Language Processing, pages 10379–10394, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.

reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics,
pages 6609–6625, Barcelona, Spain (Online). International Committee on Computational Linguistics.

Pierre Dognin, Inkit Padhi, Igor Melnyk, and Payel Das.
2021. ReGen: Reinforcement learning for text and
knowledge base generation using pretrained language
models. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
pages 1084–1099, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Marvin Hofer, Daniel Obraczka, Alieh Saeedi, Hanna
Köpcke, and Erhard Rahm. 2024. Construction of
knowledge graphs: Current state and challenges. Information, 15(8):509.
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin,
and Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learning.

Darren Edge, Ha Trinh, Newman Cheng, Joshua
Bradley, Alex Chao, Apurva Mody, Steven Truitt,
Dasha Metropolitansky, Robert Osazuwa Ness, and
Jonathan Larson. 2024. From local to global: A
graph rag approach to query-focused summarization.
arXiv preprint arXiv:2404.16130.

Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. Generalization
through memorization: Nearest neighbor language
models. In International Conference on Learning
Representations.

BV Elasticsearch. 2018. Elasticsearch. software], version, 6(1).

Mahnaz Koupaee and William Yang Wang. 2018. Wikihow: A large scale text summarization dataset. arXiv
preprint arXiv:1810.09305.

Tianyu Fan, Jingyuan Wang, Xubin Ren, and Chao
Huang. 2025. Minirag: Towards extremely simple retrieval-augmented generation. arXiv preprint
arXiv:2501.06713.

Muzhi Li, Minda Hu, Irwin King, and Ho-fung Leung.
2024a. The integration of semantic and structural
knowledge in knowledge graph entity typing. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies (Volume
1: Long Papers), pages 6625–6638, Mexico City,
Mexico. Association for Computational Linguistics.

Shangyi Geng, Wenting Zhao, and Alexander M Rush.
2025. Great memory, shallow reasoning: Limits of
kNN-LMs. In Proceedings of the 2025 Conference
of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages
471–482, Albuquerque, New Mexico. Association
for Computational Linguistics.

Yinghao Li, Haorui Wang, and Chao Zhang. 2024b.
Assessing logical puzzle solving in large language
models: Insights from a minesweeper case study. In
Proceedings of the 2024 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies
(Volume 1: Long Papers), pages 59–81, Mexico City,
Mexico. Association for Computational Linguistics.

Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao
Huang. 2024. Lightrag: Simple and fast retrievalaugmented generation.
Bernal Jiménez Gutiérrez, Yiheng Shu, Weijian Qi,
Sizhe Zhou, and Yu Su. 2025. From rag to memory:
Non-parametric continual learning for large language
models. arXiv preprint arXiv:2502.14802.

Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu
Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, Huaidong Xiong,
Lin Yuan, Jun Xu, Zaoyang Wang, Zhiqiang Zhang,
Wen Zhang, Huajun Chen, Wenguang Chen, and
Jun Zhou. 2024. Kag: Boosting llms in professional domains via knowledge augmented generation.
Preprint, arXiv:2409.13731.

Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. 2024. Hipporag: Neurobiologically inspired long-term memory for large
language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems.

Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang,
Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi
Deng, Chenyu Zhang, Chong Ruan, et al. 2024.
Deepseek-v3 technical report.
arXiv preprint
arXiv:2412.19437.

Mutian He, Tianqing Fang, Weiqi Wang, and Yangqiu
Song. 2024. Acquiring and modeling abstract commonsense knowledge via conceptualization. Artificial Intelligence, 333:104149.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2021. Measuring massive multitask language understanding. Preprint, arXiv:2009.03300.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.

Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,
and Akiko Aizawa. 2020. Constructing a multihop QA dataset for comprehensive evaluation of

Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia,
Qifan Wang, Si Zhang, Ren Chen, Chris Leung, Jiajie

10

Tang, and Jiebo Luo. 2024. LLM-rec: Personalized
recommendation via prompting large language models. In Findings of the Association for Computational
Linguistics: NAACL 2024, pages 583–612, Mexico
City, Mexico. Association for Computational Linguistics.

2024 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 311–325, Mexico City, Mexico. Association for Computational Linguistics.
Qiang Sun, Yuanyi Luo, Wenxiao Zhang, Sirui Li,
Jichunyang Li, Kai Niu, Xiangrui Kong, and Wei
Liu. 2024c. Docs2kg: Unified knowledge graph construction from heterogeneous documents assisted by
large language models. Preprint, arXiv:2406.02962.

Changsung Moon, Paul Jones, and Nagiza F Samatova.
2017a. Learning entity type embeddings for knowledge graph completion. In Proceedings of the 2017
ACM on conference on information and knowledge
management, pages 2215–2218.
Changsung Moon, Paul Jones, and Nagiza F. Samatova. 2017b. Learning entity type embeddings for
knowledge graph completion. In Proceedings of the
2017 ACM on Conference on Information and Knowledge Management, CIKM ’17, page 2215–2218, New
York, NY, USA. Association for Computing Machinery.

Xingwei Tan, Yuxiang Zhou, Gabriele Pergola, and
Yulan He. 2024. Set-aligning framework for autoregressive event temporal graph generation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies (Volume
1: Long Papers), pages 3872–3892, Mexico City,
Mexico. Association for Computational Linguistics.

Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh
Khanna, Anna Goldie, and Christopher D. Manning.
2024. Raptor: Recursive abstractive processing for
tree-organized retrieval. In International Conference
on Learning Representations (ICLR).

Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
and Ashish Sabharwal. 2022. MuSiQue: Multihop questions via single-hop question composition.
Transactions of the Association for Computational
Linguistics.

Christoph Schuhmann, Gollam Rabby, Ameya Prabhu,
Tawsif Ahmed, Andreas Hochlehnert, Huu Nguyen,
Nick Akinci Heidrich, Ludwig Schmidt, Robert Kaczmarczyk, Sören Auer, et al. 2025. Project alexandria:
Towards freeing scientific knowledge from copyright
burdens via llms. arXiv preprint arXiv:2502.19413.

Shufan Wang, Yixiao Song, Andrew Drozdov, Aparna
Garimella, Varun Manjunatha, and Mohit Iyyer.
2023a. kNN-LM does not improve open-ended text
generation. In Proceedings of the 2023 Conference
on Empirical Methods in Natural Language Processing, pages 15023–15037, Singapore. Association for
Computational Linguistics.

Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin
Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar,
Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar,
Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson,
Jacob Morrison, Niklas Muennighoff, Aakanksha
Naik, Crystal Nam, Matthew E. Peters, Abhilasha
Ravichander, Kyle Richardson, Zejiang Shen, Emma
Strubell, Nishant Subramani, Oyvind Tafjord, Pete
Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh
Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge,
and Kyle Lo. 2024. Dolma: an open corpus of
three trillion tokens for language model pretraining
research. CoRR, abs/2402.00159.

Weiqi Wang, Tianqing Fang, Wenxuan Ding, Baixuan
Xu, Xin Liu, Yangqiu Song, and Antoine Bosselut.
2023b. CAR: Conceptualization-augmented reasoner
for zero-shot commonsense question answering. In
Findings of the Association for Computational Linguistics: EMNLP 2023, pages 13520–13545, Singapore. Association for Computational Linguistics.
Weiqi Wang, Tianqing Fang, Chunyang Li, Haochen
Shi, Wenxuan Ding, Baixuan Xu, Zhaowei Wang, Jiaxin Bai, Xin Liu, Cheng Jiayang, Chunkit Chan, and
Yangqiu Song. 2024a. CANDLE: Iterative conceptualization and instantiation distillation from large
language models for commonsense reasoning. In
Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), pages 2351–2374, Bangkok, Thailand.
Association for Computational Linguistics.

Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In Proceedings of the 16th international conference
on World Wide Web, pages 697–706.
Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo
Wang, Chen Lin, Yeyun Gong, Lionel Ni, HeungYeung Shum, and Jian Guo. 2024a. Think-on-graph:
Deep and responsible reasoning of large language
model on knowledge graph. In The Twelfth International Conference on Learning Representations.

Weiqi Wang, Tianqing Fang, Haochen Shi, Baixuan Xu,
Wenxuan Ding, Liyu Zhang, Wei Fan, Jiaxin Bai,
Haoran Li, Xin Liu, and Yangqiu Song. 2024b. On
the role of entity and event level conceptualization
in generalizable reasoning: A survey of tasks, methods, applications, and future directions. Preprint,
arXiv:2406.10885.

Kai Sun, Yifan Xu, Hanwen Zha, Yue Liu, and Xin Luna
Dong. 2024b. Head-to-tail: How knowledgeable are
large language models (LLMs)? A.K.A. will LLMs
replace knowledge graphs? In Proceedings of the

Weiqi Wang, Tianqing Fang, Baixuan Xu, Chun
Yi Louis Bo, Yangqiu Song, and Lei Chen. 2023c.
CAT: A contextualized conceptualization and instantiation framework for commonsense reasoning. In

11

Bowen Zhang and Harold Soh. 2024. Extract, define,
canonicalize: An LLM-based framework for knowledge graph construction. In Proceedings of the 2024
Conference on Empirical Methods in Natural Language Processing, pages 9820–9836, Miami, Florida,
USA. Association for Computational Linguistics.

Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), pages 13111–13140, Toronto, Canada.
Association for Computational Linguistics.
Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,
and Furu Wei. 2021. MiniLMv2: Multi-head selfattention relation distillation for compressing pretrained transformers. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021,
pages 2140–2151, Online. Association for Computational Linguistics.

Hongming Zhang, Xin Liu, Haojie Pan, Haowen Ke,
Jiefu Ou, Tianqing Fang, and Yangqiu Song. 2022.
ASER: towards large-scale commonsense knowledge acquisition via higher-order selectional preference over eventualities. Artificial Intelligence,
309:103740.

Zehong Wang, Sidney Liu, Zheyuan Zhang, Tianyi Ma,
Chuxu Zhang, and Yanfang Ye. 2025. Can LLMs
convert graphs to text-attributed graphs? In Proceedings of the 2025 Conference of the Nations of
the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1412–1432,
Albuquerque, New Mexico. Association for Computational Linguistics.

Hongming Zhang, Xin Liu, Haojie Pan, Yangqiu Song,
and Cane Wing-Ki Leung. 2020. Aser: A large-scale
eventuality knowledge graph. In Proceedings of the
web conference 2020, pages 201–211.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint
arXiv:1904.09675.

Zhaowei Wang, Wei Fan, Qing Zong, Hongming Zhang,
Sehyun Choi, Tianqing Fang, Xin Liu, Yangqiu Song,
Ginny Wong, and Simon See. 2024c. AbsInstruct:
Eliciting abstraction ability from LLMs through explanation tuning with plausibility estimation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 973–994, Bangkok, Thailand. Association for Computational Linguistics.

Wenting Zhao, Ye Liu, Tong Niu, Yao Wan, Philip
Yu, Shafiq Joty, Yingbo Zhou, and Semih Yavuz.
2024. DIVKNOWQA: Assessing the reasoning ability of LLMs via open-domain question answering
over knowledge base and text. In Findings of the
Association for Computational Linguistics: NAACL
2024, pages 51–68, Mexico City, Mexico. Association for Computational Linguistics.
Chang Zong, Yuchen Yan, Weiming Lu, Jian Shao,
Yongfeng Huang, Heng Chang, and Yueting Zhuang.
2024. Triad: A framework leveraging a multi-role
LLM-based agent to solve knowledge base question
answering. In Proceedings of the 2024 Conference
on Empirical Methods in Natural Language Processing, pages 1698–1710, Miami, Florida, USA. Association for Computational Linguistics.

Zhaowei Wang, Haochen Shi, Weiqi Wang, Tianqing
Fang, Hongming Zhang, Sehyun Choi, Xin Liu, and
Yangqiu Song. 2024d. AbsPyramid: Benchmarking
the abstraction ability of language models with a unified entailment graph. In Findings of the Association
for Computational Linguistics: NAACL 2024, pages
3991–4010, Mexico City, Mexico. Association for
Computational Linguistics.
Yike Wu, Yi Huang, Nan Hu, Yuncheng Hua, Guilin Qi,
Jiaoyan Chen, and Jeff Z. Pan. 2024. CoTKR: Chainof-thought enhanced knowledge rewriting for complex knowledge graph question answering. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 3501–
3520, Miami, Florida, USA. Association for Computational Linguistics.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. 2018. HotpotQA: A dataset
for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Hongbin Ye, Honghao Gui, Xin Xu, Xi Chen, Huajun
Chen, and Ningyu Zhang. 2023. Schema-adaptable
knowledge graph construction. In Findings of the
Association for Computational Linguistics: EMNLP
2023, pages 6408–6431, Singapore. Association for
Computational Linguistics.

12

A

Prompts for Triple Extractions

Event and Entity Triple Extraction
Please analyze and summarize the participation relations between the events and entities in the given
paragraph. Each event is a single independent sentence. Additionally, identify all the entities that participated in the events. Do not use ellipses. Please
strictly output in the following JSON format:
[
{
"Event": "{a simple sentence describing an
event}",
"Entity": ["{entity 1}", "{entity 2}", "..."]
}
...
]

Entity Relationship Extraction
Given a passage, summarize all the important entities
and the relations between them in a concise manner.
Relations should briefly capture the connections between entities, without repeating information from
the head and tail entities. The entities should be as
specific as possible. Exclude pronouns from being
considered as entities. The output should strictly adhere to the following JSON format:
[
{
"Head": "{a noun}",
"Relation": "{a verb}",
"Tail": "{a noun}",
}
...
]
Here is the passage:

Figure 3: The figure demonstrates the prompts we use
to generate the text triples describing relations between
entities and events.

format. To manage the constraints of LLM input
capacity, denoted as Lmax tokens, we preprocess
the text corpus to ensure compatibility, segmenting documents as needed and organizing them into
batches for efficient processing. This section outlines the preprocessing strategy, the staged extraction process, and key implementation details.

Figure 2: The figure demonstrates the prompts we use
to generate the text triples describing relations between
entities.

The prompts used for extracting entity-entity,
entity-event, and event-event triples are given in
Figure 2, Figure 3, and Figure 4 respectively.

B

Implementation Details of Knowledge
Graph Construction Framework

B.1.1

In this section, we elaborate on the process
of fully automating knowledge graph construction. Given a collection of n documents D =
{D1 , D2 , D3 , . . . , Dn }, our method systematically
builds the graph.
B.1

Text Preprocessing and Data
Organization

Given a corpus D = {D1 , D2 , . . . , Dn } of n documents, we first filter the dataset to include only
English-language texts, identified through metadata or assumed if unspecified, to match the linguistic capabilities of our LLMs. To adhere to
the token limit Lmax , we account for an instructional prompt length, Linst derived from empirical observations. The maximum token length per
text segment, Cmax , is calculated as: Cmax =
(Lmax − Linst )
Documents exceeding Cmax are divided into
smaller chunks, each tagged with a unique identifier and metadata to maintain traceability. This
segmentation ensures that inputs remain within the
LLM’s token capacity, preserving contextual integrity without truncation.
The preprocessed text is then grouped into
batches of size B, utilizing a custom data management framework that integrates with standard
dataset loading tools. Tokenization is applied to
each batch, adjusting for padding and truncation to
produce consistent input representations suitable
for LLM processing.

Triple Extraction

Our approach to triple extraction employs a multiphase pipeline that utilizes the generative power
of Large Language Models (LLMs) to convert unstructured text into structured knowledge triples,
drawing from the Dolma corpus (Soldaini et al.,
2024). This pipeline systematically extracts three
categories of relationships—Entity-Entity, EntityEvent, and Event-Event—forming the foundation
of a comprehensive knowledge graph. Designed
for scalability and resilience, the method incorporates batch processing, text segmentation, and
robust output parsing to efficiently process largescale datasets.
The extraction unfolds across three sequential
stages, each tailored to a specific relationship
type, leveraging a single LLM guided by distinct
prompts to produce structured outputs in JSON
13

v ∈ VE (event nodes), and r ∈ R. This bidirectional extraction captures both entities involved in
events and events tied to entities, enhancing the
graph’s relational depth.

Event Relationship Extraction
Please analyze and summarize the relationships between the events in the paragraph. Each event is a
single independent sentence. Identify temporal and
causal relationships between the events using the following types: before, after, at the same time, because,
and as a result. Each extracted triple should be specific, meaningful, and able to stand alone. Do not
use ellipses. The output should strictly adhere to the
following JSON format:
[
{
"Head": "{a simple sentence describing the
event 1}",
"Relation": "{temporal or causality relation between the events}",
"Tail": "{a simple sentence describing the event
2}"
}
...
]

B.1.4

The third stage targets Event-Event relationships,
as shown in Figure 4„ detecting causal, temporal,
or logical connections between events. A specialized prompt, PV V , is applied to the text segments,
prompting the LLM to generate triples of the form
(v1 , r, v2 ), where v1 , v2 ∈ VE and r ∈ R. The
parsing process follows the same methodology, repairing JSON outputs as needed. To accommodate
potentially intricate event descriptions, we extend
the generation limit to Lext = α · Lmax , where
α > 1 is a scaling factor, ensuring comprehensive
capture of event interactions.

Figure 4: The figure demonstrates the prompts we use
to generate the text triples describing relations between
events.

B.1.2

B.1.5

Implementation Considerations

The pipeline supports a variety of LLMs, including models from Google, Meta, Mistral, Microsoft,
and others, configured with optimized precision settings (e.g., bfloat16 or float16) and enhanced with
acceleration techniques where applicable. Deployment occurs on GPUs, with input-output formatting
governed by model-specific chat templates, Tchat ,
to ensure compatibility. The extracted triples, along
with their corresponding texts and metadata, are
serialized into JSON files per batch, enabling subsequent schema induction and evaluation.
This multi-stage pipeline achieves thorough
triple extraction by addressing each relationship
type systematically, harnessing the LLM’s generative capabilities within a scalable and fault-tolerant
framework. The use of variables such as Lmax ,
Cmax , and B ensures flexibility across different
models and datasets, reinforcing the methodology’s
adaptability and generalizability.

Stage 1: Extraction of Entity-Entity
Relationships

In the initial stage, as shown in Figure 2, we extract
Entity-Entity relationships, identifying connections
between named entities such as individuals, organizations, or locations. For each batch, we prepend a
system prompt, PEE , which instructs the LLM to
detect entities and their interrelations, followed by
the segmented text. The LLM generates a response
in JSON format, which is subsequently decoded
and parsed. The parsing process isolates the structured content by locating the model’s answer start
token, Tstart , repairs any malformed JSON, and
extracts a list of dictionaries. Each dictionary represents a triple (e1 , r, e2 ), where e1 , e2 ∈ VN are
entity nodes and r ∈ R is a relation type. If parsing encounters errors, an empty list is returned to
ensure pipeline continuity .

B.2
B.1.3

Stage 3: Extraction of Event-Event
Relationships

Stage 2: Extraction of Entity-Event
Relationships

Schema Induction

Following the extraction of knowledge triples, our
methodology advances to schema induction, a critical step that abstracts specific entities, events, and
relations into generalized types to form a coherent and adaptable schema for the knowledge graph.
This process leverages the contextual understanding of Large Language Models (LLMs) to generate
conceptual phrases that represent the types or related concepts of each graph element, enabling the
graph to scale across diverse domains without man-

The second stage focuses on Entity-Event relationships, as shown in Figure 3, linking entities to specific occurrences or events. Using the original text
segments from Stage 1, we apply a new prompt,
PEV , directing the LLM to identify events and
their associated entities. The generation and parsing steps mirror those of Stage 1, producing triples
of the form (e, r, v) or (v, r, e), where e ∈ VN ,
14

ual schema design. The induced schema aligns
with the formal definition of a knowledge graph
G = (V, E, C, ϕ, ψ), where C denotes the set of
concepts, and ϕ and ψ map nodes and relations to
subsets of C, respectively.
Our schema induction pipeline processes the
triples extracted from the Dolma corpus (Soldaini
et al., 2024), organizing them into batches and employing a generative approach to derive abstract
representations. The process targets three components—events (VE ), entities (VN ), and relations
(R)—producing a set of conceptual phrases for
each, which collectively form the concept set C.
This section outlines the abstraction methodology,
the role of context in entity conceptualization, and
key implementation details.

two words, that abstractly represent the input element. These phrases must satisfy several criteria:
they should encapsulate the element’s type or related concepts, vary in abstraction level, and avoid
repetition or inclusion of the original input term.
For each element, a minimum of three phrases is
targeted, though more may be generated depending
on the LLM’s output.
For events (v ∈ VE ), the prompt directs the LLM
to identify abstract event types or related notions.
For example, an event such as "Sam playing with
his dog" might yield phrases like "playing," "bonding," and "relaxing event," reflecting different levels of generality. For entities (e ∈ VN ), the prompt
similarly elicits abstract entity types, augmented
by contextual information derived from the graph
structure, as detailed below. Relations (r ∈ R) are
abstracted into phrases that capture their semantic essence, such as transforming "participated in"
into "engage in," "attend," and "involve in." The
LLM generates these outputs in a structured format,
which we parse into lists of phrases, forming the
mappings ϕ(v), ϕ(e), and ψ(r) to the concept set
C.
The abstraction process operates in batches, processing Bs elements simultaneously. The input
prompts are tokenized to a maximum length Ltok ,
and the LLM generates responses under controlled
parameters (e.g., temperature τ and top-p sampling
with probability p) to balance creativity and coherence. The resulting phrases are stored alongside
their corresponding elements, ensuring traceability
and enabling subsequent analysis.

Abstract Event Phrase Generation
I will give you an EVENT. You need to give several
phrases containing 1-2 words for the ABSTRACT
EVENT of this EVENT. You must return your answer in the following format: phrases1, phrases2,
phrases3,... You can’t return anything other than answers. These abstract event words should fulfill the
following requirements:
1. The ABSTRACT EVENT phrases can well
represent the EVENT, and it could be the type of the
EVENT or the related concepts of the EVENT.
2. Strictly follow the provided format, do not add
extra characters or words.
3. Write at least 3 or more phrases at different
abstract level if possible.
4. Do not repeat the same word and the input in
the answer.
5. Stop immediately if you can’t think of any more
phrases, and no explanation is needed.
Examples:
EVENT: A man retreats to mountains and forests
Your answer: retreat, relaxation, escape, nature, solitude
EVENT: A cat chased a prey into its shelter Your
answer: hunting, escape, predation, hidding, stalking
EVENT: Sam playing with his dog Your answer: relaxing event, petting, playing, bonding, friendship
EVENT: [EVENT] Your answer:

B.2.2

As shown in Figure 6, to enhance the accuracy of
entity abstraction, we incorporate contextual information extracted from the knowledge graph. For
each entity e ∈ VN , we examine its neighboring
nodes—predecessors and successors—along with
their associated relations. A subset of these neighbors, limited to Nctx (e.g., one predecessor and one
successor), is randomly sampled to construct a context string. This string concatenates the neighbor’s
identity and relation (e.g., "neighbor1 relation1, relation2 neighbor2"), providing the LLM with additional semantic cues. For instance, an entity "Black
Mountain College" with context "started by John
Andrew Rice" might yield phrases like "college,"
"school," and "liberal arts college." This contextual enrichment ensures that the abstracted types
are grounded in the entity’s role within the graph,

Figure 5: This figure shows the prompt used for generating the concepts for an event.

B.2.1

Contextual Enhancement for Entities

Abstraction Methodology

The schema induction begins by categorizing the
nodes and edges of the knowledge graph G into
events, entities, and relations. For each category,
we process the elements in batches of size Bs to
optimize computational efficiency and scalability.
The LLM is prompted with tailored instructions to
generate a list of phrases, each containing one to
15

improving the schema’s relevance and specificity.

Abstract Relation Phrase Generation
I will give you a RELATION. You need to give
several phrases containing 1-2 words for the ABSTRACT RELATION of this RELATION. You must
return your answer in the following format: phrases1,
phrases2, phrases3,... You can’t return anything other
than answers. These abstract intention words should
fulfill the following requirements:
1. The ABSTRACT RELATION phrases can well
represent the RELATION, and it could be the type
of the RELATION or the simplest concepts of the
RELATION.
2. Strictly follow the provided format, do not add
extra characters or words.
3. Write at least 3 or more phrases at different
abstract level if possible.
4. Do not repeat the same word and the input in
the answer.
5. Stop immediately if you can’t think of any more
phrases, and no explanation is needed.
Examples:
RELATION: participated in Your answer: become
part of, attend, take part in, engage in, involve in
RELATION: be included in Your answer: join, be a
part of, be a member of, be a component of
RELATION: [RELATION] Your answer:

Abstract Entity Phrase Generation
I will give you an ENTITY. You need to give several
phrases containing 1-2 words for the ABSTRACT
ENTITY of this ENTITY. You must return your answer in the following format: phrases1, phrases2,
phrases3,... You can’t return anything other than answers. These abstract intention words should fulfill
the following requirements:
1. The ABSTRACT ENTITY phrases can well
represent the ENTITY, and it could be the type of the
ENTITY or the related concepts of the ENTITY.
2. Strictly follow the provided format, do not add
extra characters or words.
3. Write at least 3 or more phrases at different
abstract level if possible.
4. Do not repeat the same word and the input in
the answer.
5. Stop immediately if you can’t think of any more
phrases, and no explanation is needed.
Examples:
ENTITY: Soul CONTEXT: premiered BFI London
Film Festival, became highest-grossing Pixar release
Your answer: movie, film
ENTITY: Thinkpad X60 CONTEXT: Richard Stallman announced he is using Trisquel on a Thinkpad
X60 Your answer: Thinkpad, laptop, machine, device,
hardware, computer, brand
ENTITY: Harry Callahan CONTEXT: bluffs another
robber, tortures Scorpio Your answer: person, Amarican, character, police officer, detective
ENTITY: Black Mountain College CONTEXT: was
started by John Andrew Rice, attracted faculty Your
answer: college, university, school, liberal arts college
ENTITY: 1st April CONTEXT: Utkal Dibas celebrates Your answer: date, day, time, festival
ENTITY: [ENTITY] CONTEXT: [CONTEXT] Your
answer:

Figure 7: This figure shows the conceptualization
prompts for relations enhanced with context.

selected to reduce processing time during experimentation.
The LLM, configured with a precision setting
(e.g., float16) and optimized with acceleration techniques, operates on a GPU to handle the batched
inference efficiently. Prompts are formatted using a
model-specific chat template, Tchat , ensuring compatibility with the LLM’s input-output conventions.
The generated phrases are written to a CSV file,
with each row recording the original element, its
abstracted phrases, and its type (event, entity, or
relation). Post-processing aggregates these phrases
to compute the unique concepts in C, providing
statistics on the schema’s coverage, such as the
number of distinct event types, entity types, and
relation types.

Figure 6: This figure shows the conceptualization
prompts for entities enhanced with context.

Events and relations, as shown in Figure 5 and
Figure 7, by contrast, rely solely on their textual
descriptions without additional context, as their abstraction focuses on inherent semantics rather than
graph connectivity. This distinction reflects the differing roles of nodes and edges in the knowledge
graph structure.
B.2.3

Implementation Details

The schema induction pipeline processes a graph
G serialized from the triple extraction phase, typically stored in a binary format and loaded into memory. The elements are partitioned into batches, with
the option to apply slicing (dividing the workload
into Stotal slices and processing the Sslice -th portion) for distributed computation. If a sample size
Nsample is specified, a random subset of batches is

This approach yields a flexible and automated
schema, mapping each node v ∈ V and relation
r ∈ R to a subset of concepts in C via ϕ and ψ. By
abstracting specific instances into general types, the
induced schema enhances the knowledge graph’s
adaptability, supporting downstream applications
across varied domains without requiring manual
curation.
16

C
C.1

Experiment Settings of Schema
Accuracy
Datasets

BS(t, t̂) = 2

(3)

BS-R(T , T̂ ) =

BS-C(St , St̂ ) =

1 X

max BS(t, t̂),

(4)

1 X
max BS(t, t̂),
t∈St
|St̂ |

(5)

|T̂ |

t∈T

t̂∈T̂

t̂∈St̂

where T and T̂ represent a set of ground truth
types and induced schemas in each testing instance,
respectively. Similarly, St and St̂ denote the set of
ground truth types and induced schemas across the
entire testing set, respectively.

D

Case Study Examples

Figures 9 and 10 demonstrate specific cases where
events and concepts are crucial for effective knowledge graph utilization in retrieval-augmented generation. Figure 9 illustrates how event nodes provide essential contextual information that entityonly representations miss, while Figure 10 showcases how concept nodes establish semantic bridges
across otherwise disconnected subgraphs, enabling
more comprehensive reasoning for complex questions.

E

Algorithm for RAG

We include all the algorithms used in our RAG
evaluation on various graphs constructed by AutoSchemaKG. Algorithm 1 presents the Think-onGraph reasoning method that leverages our knowledge graphs for multi-hop question answering. For
adapting our entity-event-concept graphs at different scales, we implemented two variants of HippoRAG2: Algorithm 2 for smaller, more focused
graph traversal, and Algorithm 3 for large-scale
graph exploration with optimized memory management. These adaptations enable efficient navigation of the rich semantic structures in our ATLAS
knowledge graphs.

We employ BertScore-Recall and BertScoreCoverage as the evaluation metrics, which are denoted as BS-R and BS-C respectively. They are
used to calculate how many types in each instance
or entire testing set are recalled by our schema
induction method. The BertScore (Zhang et al.,
2019), which is denoted as BS, between each pair
of type and induced schema are calculated as follows:
1 X
max x⊤ x ,
|t| t ∈t t̂j ∈t̂ ti t̂j

BertRecall · BertPrec
,
BertRecall + BertPrec

where t and t̂ represents the tokens of a ground truth
type and induced schema, respectively. The embedding vector of each token ti or t̂j of a type t or induced schema t̂ is denoted as xti and xt̂j , which are
obtained with RoBERTa (Liu et al., 2019). Then
the BS-R and BS-C can be calculated as follows:

Metrics

BertRecall =

(2)

t̂i ∈t̂

Entity Typing. We conduct experiments on
the typed entities of two real-world knowledge
graphs, FB15kET (Bordes et al., 2013) and
YAGO43kET (Moon et al., 2017a), which are the
subsets of Freebase (Bollacker et al., 2008) and
YAGO (Suchanek et al., 2007), respectively. The
types of entities are collected from (Moon et al.,
2017b). There are 3,584 and 45,182 entity types
in FB15kET and YAGO43kET, respectively. We
utilize the entities in the testing sets of these two
datasets with their types as ground truths to validate
the entity induction performance of our schema induction method.
Event Typing. We conduct experiments on
the typed events of wikiHow (Koupaee and Wang,
2018), which is an online community contains a
collection of professionally edited how-to guideline articles. The types of events are collected by
P2GT (Chen et al., 2020). There are 625 event
types among 12,795 events. We utilize the events
in the testing set of wikiHow with their types as
ground truths to validate the event induction performance of our schema induction method.
Relation Typing. There are no datasets designed for the relation typing task, so here we make
use of the domain segments separated by "/" in
FB15kET (Bordes et al., 2013) to extract the types.
These domain segments serve as ground truth types,
with the last domain component functioning as the
relation itself. There are 607 relation types among
1,345 relations in FB15kET. We utilize the relations in the testing set of FB15kET with their types
as ground truths to validate the relation induction
performance of our schema induction method.
C.2

1 X
max x⊤
tj xt̂i ,
tj ∈t
|t̂|

BertPrec =

(1)

i

17

Algorithm 1 Think on Graph (ToG) (Sun et al., 2024a) for Question Answering
Require: Knowledge Graph G, Query q, Top-N parameter, Maximum depth Dmax
Ensure: Answer to query q
1: Extract entities from query q using NER
2: Retrieve top-k initial nodes from G based on entity similarity
3: Let P ← set of paths, each containing a single initial node
4: D ← 0
▷ Current search depth
5: while D ≤ Dmax do
6:
P ← Search(q, P, G)
▷ Expand paths by one hop
7:
P ← Prune(q, P, N )
▷ Keep top-N most relevant paths
8:
if Reasoning(q, P ) determines paths sufficient then
9:
return Generate(q, P )
▷ Generate answer using paths
10:
end if
11:
D ←D+1
12: end while
13: return Generate(q, P )
▷ Generate answer using best available paths
14: procedure S EARCH(q, P, G)
15:
Pnew ← ∅
16:
for each path p ∈ P do
17:
etail ← last entity in path p
18:
S ← successors of etail in G not already in p
19:
R ← predecessors of etail in G not already in p
20:
if S = ∅ and R = ∅ then
21:
Pnew ← Pnew ∪ {p}
▷ Keep dead-end paths
22:
else
23:
for each node n ∈ S do
24:
r ← relation from etail to n in G
25:
Pnew ← Pnew ∪ {p + [r, n]}
▷ Extend path forward
26:
end for
27:
for each node n ∈ R do
28:
r ← relation from n to etail in G
29:
Pnew ← Pnew ∪ {p + [r, n]}
▷ Extend path backward
30:
end for
31:
end if
32:
end for
33:
return Pnew
34: end procedure
35: procedure P RUNE(q, P, N )
36:
Score each path in P using LLM relevance assessment (1-5 scale)
37:
Sort paths by decreasing score
38:
return top-N highest scoring paths
39: end procedure
40: procedure R EASONING(q, P )
41:
Extract triples from paths in P
42:
Ask LLM if triples are sufficient to answer q (Yes/No)
43:
return True if answer is "Yes", False otherwise
44: end procedure
45: procedure G ENERATE(q, P )
46:
Extract triples from paths in P
47:
Prompt LLM with triples and query q to generate answer
48:
return generated answer
49: end procedure
18

Algorithm 2 HippoRAG2 (Gutiérrez et al., 2025)
General algorithm follows the original implementation, while we modify the initialization of graph and
embeddings.
1: function INIT(graph_type)

if graph_type is entity then
graph, embedding ← Graph(entity), Embeddings(entity)
4:
else if graph_type is entity+event then
5:
graph, embedding ← Graph(entity, event), Embeddings(entity, event)
6:
else if graph_type is entity+event+concept then
7:
graph, embedding ← Graph(entity, event, concept), Embeddings(entity, event, concept)
8:
end if
9: end function
10: function QUERY 2 EDGE(query, topN)
11:
Qemb ← Retriever(query)
12:
S = Q · We
▷ Calculate similarity scores with precomputed edge embeddings
13:
E = argsorti (S)[: N ]
▷ Select topN edges based on scores
14:
filtered_edges ← LLM_filter(E)
▷ Filter edges using Large Language Model
15:
mapped_edges ← Map_edges(filtered_edges)
▷ Map filtered edges to original edges
16:
return_node_scores ← Calculate_node_scores(mapped_edges)
17:
return return_node_scores
18: end function
19: function QUERY 2 PASSAGE(query, weight_adjust)
20:
Qpass ← Encode(query)
▷ Encode query into passage representation
21:
Stext ← Similarity_Scores(Qpass , text_embeddings)
22:
return Scores_Dictionary(Stext )
23: end function
24: function RETRIEVE _ PERSONALIZATION _ DICT(query, topN)
25:
node_dict ← query2edge(query, topN )
26:
text_dict ← query2passage(query, weight_adjust)
27:
return node_dict, text_dict
28: end function
29: function RETRIEVE _ PASSAGES(query, topN)
30:
node_dict, text_dict ← retrieve_personalization_dict(query, topN )
31:
if node_dict is empty then
32:
return TopN_Text_Passages(text_dict)
33:
else
34:
personalization_dict ← {node_dict, text_dict}
35:
page_rank_scores ← PageRank(personalization_dict)
36:
return TopN_Passages(page_rank_scores)
37:
end if
38: end function
2:

3:

19

Algorithm 3 LargeKGRetriever
A variant of HippoRAG2 (Gutiérrez et al., 2025), optimized with dynamic graph sampling and common
word filtering
1: function INIT(graph_type)

keyword ← Default_graph_keyword
▷ Keyword can be cc, pes2o, wiki
3:
Initialize_resources(keyword)
4:
Load_node_and_edge_indexes()
5: end function
6: function RETRIEVE _ TOPK _ NODES(query, top_k_nodes)
7:
entities ← LLM_NER(query)
8:
KG_entities ← Encode_and_Search(entities, FAISS_index)
9:
filtered_keywords ← LLM_filter(KG_entities)
10:
return filtered_keywords
11: end function
12: function RETRIEVE _ PERSONALIZATION _ DICT(query, number_of_source_nodes)
13:
topk_nodes ← retrieve_topk_nodes(query, number_of_source_nodes)
14:
if topk_nodes == {} then
15:
return {}
16:
end if
17:
Update personalization dictionary with topk_nodes
18:
return Personalization dictionary
19: end function
20: function PAGERANK (personalization_dict, topN, sampling_area)
21:
GSample ← Random Walk with Restart Sampling
22:
Scores = PageRank(GSample , personalization_dict)
23:
topN_nodes = argsorti (Scores)[: N ]
24:
for node in topN nodes do
25:
Connected_Passage += node.score
26:
end for
27:
return TopN_Ranked_Passages
28: end function
29: function RETRIEVE _ PASSAGES(query, topN, number_of_source_nodes, sampling_area)
30:
personalization_dict ← retrieve_personalization_dict(query, number_of_source_nodes)
31:
if personalization_dict is empty then
32:
return {}, [0]
33:
end if
34:
topN_passages ← pagerank(personalization_dict, topN, sampling_area)
35:
return topN_passages
36: end function
2:

20

F

The Recall Metrics in Opendomain QA
Tasks

same across different knowledge bases. The detailed results of the 3 domains are shown in Table 10.

We also use Retrieval Quality metrics at k ∈ {2, 5}:
PR@k = |Dk ∩ S|/|S| where PR@k (Partial Recall) measures the fraction supporting document is
in top-k, Dk is the set of top-k retrieved documents,
and S is the set of supporting documents. For multihop QA datasets (HotpotQA, 2WikiMultihopQA,
MuSiQue), these retrieval metrics are crucial as
they measure how effectively our system retrieves
the evidence needed for multi-step reasoning.
Questions in datasets like 2WikiMultihopQA
(Ho et al., 2020) and HotpotQA (Yang et al., 2018)
tend to be more entity-centric, with relationships
and entities more explicitly represented, which aids
retrievers in easily locating relevant subgraphs. In
contrast, MuSiQue (Trivedi et al., 2022), due to its
questions’ increased complexity in both description
and multi-hop nature, poses greater challenges for
retrieval. Additionally, differences in graph construction cause the retrievers to perform differently
across datasets.

G

Details and Full Results on General
Benchmarks

G.1

Implementation and Evaluation Details
on FELM

G.2

Table 11 presents our classification for organizing
MMLU tasks into distinct subject categories, providing a structured framework for domain-specific
performance analysis. Table 12 displays comprehensive results across all MMLU subject areas,
revealing an important insight: while retrievalaugmented generation enhances performance in
knowledge-intensive domains, it can negatively impact performance on reasoning-focused tasks such
as mathematics and logical reasoning. This finding
aligns with previous research suggesting that RAG
may sometimes interfere with LLMs’ inherent reasoning capabilities.

For the evaluation metrics, we follow the original
paper (Chen et al., 2023) and use balanced accuracy
and F1 score to evaluate the factuality checking
capability. For the classification of segments in an
instance, we ask the model to generate the ID of
false segments, and then get the true positive (TP),
false positive (FP), true negative (TN) and false
negative (FN) results. The balanced accuracy is
calculated as:
Balanced Accuracy =

TP
TN
+
TP + FN
TN + FP

(6)

Since we use F1 score to evaluate the factual error
detection capability, we calculate the F1 score as:
F1 =

2 · Precision · Recall
Precision + Recall

Implementation and Evaluation Details
on MMLU

(7)

N
TN
where Precision = T NT+F
N and Recall = T N +F P .
We use the Retrieval-Augmented Generation
method with different knowledge bases on 3 domains (world knowledge, science and technology,
and writing/recommendation) of FELM benchmark. For the math domain and reasoning domain,
we use the vanilla setting and their results are the

21

Model/Dataset

MuSiQue

Metric

2Wiki

HotpotQA

Recall@2

Recall@5

Recall@2

Recall@5

Recall@2

Recall@5

34.8
32.4

46.6
43.5

46.6
55.3

57.5
65.3

58.4
57.3

75.3
74.8

48.1
49.7
52.7

63.6
65.9
69.7

66.7
67.3
67.1

74.8
76.0
76.5

75.8
79.2
84.1

89.1
92.4
94.5

47.0
41.2
56.1

57.8
53.2
74.7

58.3
71.9
76.2

66.2
90.4
90.4

76.8
60.4
83.5

86.9
77.3
96.3

61.72
61.37
61.08

75.45
74.56
71.9

51.89
51.31
52.8

65.95
65.93
65.4

67.34
68.59
68.46

84.25
85.85
84.6

77.59
81.26
84.17

92.16
92.66
93.04

Baseline Retrievers
Contriever
BM25
LLM Embeddings
GTE-Qwen2-7B-Instruct
GritLM-7B
NV-Embed-v2 (7B)
Existing Graph-based RAG Methods
RAPTOR (Llama-3.3-70B-Instruct)
HippoRAG (Llama-3.3-70B-Instruct)
HippoRAG2 (Llama-3.3-70B-Instruct)

AutoSchemaKG (LLama-3.1-8B-Instruct) + HippoRAG1
Entity-KG (Llama-3-8B-Instruct)
Entity-Event-KG (Llama-3-8B-Instruct)
Full-KG (Llama-3-8B-Instruct)

41.37
41.28
40.78

51.08
51.12
50.36

AutoSchemaKG (LLama-3.1-8B-Instruct) + HippoRAG2
Entity-KG (Llama-3-8B-Instruct)
Entity-Event-KG (Llama-3-8B-Instruct)
Full-KG (Llama-3-8B-Instruct)

48.33
48.83
49.12

72.58
72.7
72.48

Table 8: Recall @ 2 and Recall @ 5.
Table 9: Recall performance in the knowledge graph created by Llama-3-8B-Instruct shows strong performance that
is comparable with the knowledge graph created with 70B model.

World Knowledge

Science and Technology

Writing/Recommendation

Corpus

Method
P

R

F1

Acc

P

R

F1

Acc

P

R

F1

Acc

-

-

36.67

29.93

32.96

55.10

15.43

24.51

18.94

50.49

25.95

12.73

17.09

52.69

Wikipedia

Random
BM25
Dense Retrieval

31.78
26.82
33.93

27.89
32.65
38.78

29.71
29.45
36.19

52.52
49.31
54.97

7.95
15.23
16.92

11.76
38.24
43.14

9.49
21.79
24.31

43.94
50.48
53.01

21.95
29.21
25.22

26.97
48.69
43.45

24.20
36.52
31.91

53.78
62.40
58.68

Pes2o-Abstract

Random
BM25
Dense Retrieval

27.36
32.43
31.43

19.73
32.65
29.93

22.92
32.54
30.66

49.86
53.34
52.50

10.13
11.18
20.51

15.69
17.65
47.06

12.31
13.69
28.57

45.64
46.54
57.55

26.92
26.75
25.37

28.84
32.96
32.21

27.85
29.53
28.38

56.50
57.34
56.51

Common Crawl

Random
BM25
Dense Retrieval

30.14
27.21
25.50

29.93
27.21
34.69

30.03
27.21
29.39

51.72
49.71
48.00

11.17
12.21
15.48

21.57
25.49
36.27

14.72
16.51
21.70

45.75
46.68
50.78

23.73
27.32
24.41

28.09
40.82
38.95

25.73
32.73
30.01

54.91
59.42
57.27

Think on Graph

26.00

8.84

13.20

49.62

23.76

23.53

23.65

55.15

42.86

12.36

19.19

54.51

HippoRAG2

33.33
39.17
33.80

42.18
31.97
48.98

37.24
35.21
40.00

54.98
56.51
56.18

16.45
21.35
18.06

50.00
40.20
52.94

24.76
27.89
26.93

52.75
57.13
55.42

32.82
30.37
24.20

32.21
15.36
25.47

32.51
20.40
24.82

59.43
54.11
54.66

Text Corpora

Knowledge Graph
Freebase
ATLAS-Wiki
ATLAS-Pes2o
ATLAT-CC

Table 10: Factuality results (%) on different domains of FELM benchmark with different Text Corporas and retrieval
methods. P, R, F1, and Acc denote Precision, Recall, F1 score, and Balanced Accuracy, respectively.

22

Subject

Task

History

high school european history, high school us history, high school world history,
prehistory

Formal Logic

formal logic, logical fallacies

Law

international law, jurisprudence, professional law

Philosophy and Ethics

philosophy, moral disputes, moral scenarios, business ethics

Religion

world religions

Medicine and Health

clinical knowledge, college medicine, medical genetics, professional medicine, virology, human aging, nutrition, anatomy

Social Sciences

high school geography, high school government and politics, high school psychology,
professional psychology, sociology, human sexuality, us foreign policy, security
studies

Economics

high school macroeconomics, high school microeconomics, econometrics

Business and Management

management, marketing, professional accounting, public relations

Math

abstract algebra, college mathematics, elementary mathematics, high school mathematics, high school statistics

Natural Sciences

astronomy, college biology, college chemistry, college physics, conceptual physics,
high school biology, high school chemistry, high school physics

Computer Science and Engineering

college computer science, high school computer science, computer security, electrical
engineering, machine learning

Global Facts

global facts, miscellaneous

Table 11: The correspondence between subjects and tasks.

MMLU
overall History Law Religion PaE MaH GF BaM SS Logic Econ Math NS CSaE
None
69.18 76.59 66.86 83.04 63.55 70.38 66.72 72.20 79.74 64.35 68.35 57.31 65.27 66.70
Freebase-ToG
70.36 78.42 69.00 75.44 65.67 72.65 67.27 73.67 76.00 66.03 67.34 60.56 69.39 68.23
Random + KB
Wikipedia
68.06 76.64 66.82 79.53 59.26 70.34 66.46 67.34 77.78 59.21 65.35 60.91 67.52 61.87
Common Crawl 67.93 74.89 66.52 79.53 61.74 69.82 68.11 67.67 77.52 59.30 64.20 59.80 67.42 62.22
Pes2o-Abstract 68.07 76.24 64.16 80.70 62.01 70.62 66.59 69.27 77.16 62.39 64.70 60.07 66.41 62.18
BM25 + KB
Wikipedia
68.99 76.67 67.35 78.36 63.34 69.35 61.98 71.39 76.99 62.30 65.56 61.67 69.31 65.60
Common Crawl 68.33 76.15 66.36 80.12 60.43 69.58 64.67 69.47 76.71 63.18 68.22 62.26 65.55 65.04
Pes2o-Abstract 69.04 78.01 65.89 78.95 63.83 71.01 65.78 68.29 77.07 59.34 68.87 61.20 67.73 65.74
Dense Retrieval + KB
Wikipedia
69.37 73.59 69.60 79.53 63.58 70.82 62.41 72.57 76.83 62.21 67.35 61.79 69.81 65.39
Common Crawl 67.03 74.47 68.98 79.53 60.46 69.29 64.09 68.88 75.21 61.86 62.27 57.13 64.54 64.47
Pes2o-Abstract 69.07 75.79 61.82 78.36 65.15 69.72 66.77 69.02 76.47 63.05 63.07 63.92 69.53 67.86
HippoRAG2 + KG
Wikipedia
68.22 76.73 67.38 84.21 66.01 70.82 68.36 72.35 79.16 63.65 64.53 50.09 65.45 62.10
Common Crawl 68.26 78.16 70.85 83.04 65.60 71.28 63.95 68.84 78.16 65.42 67.51 52.87 63.32 63.40
Pes2o-Abstract 69.19 77.13 68.41 81.29 65.05 72.75 65.67 72.32 81.19 62.98 65.29 54.24 65.41 64.04
Corpus

Table 12: Performance comparison of our knowledge graph (KG) integrated with HippoRAG2 against baseline
retrieval methods (Random, BM25, Dense Retrieval) across Wikipedia, Common Crawl, and Pes2o-Abstract corpora
on MMLU benchmarks. Tasks are classified according to subjects, with bold and underline indicating the highest and
the second highest performance. PaE, MaH, GF, BaM, SS, Econ, NS and CSaE represent Philosophy_and_Ethics,
Medicine_and_Health, Global_Facts, Business_and_Management, Social_Sciences, Economics, Natural_Sciences
and Computer_Science_and_Engineering respectively.

23

Multiple-Choice Question Generation and Answering
MCQ Generation Prompt:
You are an expert in generating multiple-choice questions (MCQs) from scientific texts. Your task is to
generate 5 multiple-choice questions based on the
following passage.
Each question should:
- Focus on factual claims, numerical data, definitions, or relational knowledge from the passage.
- Have 4 options (one correct answer and three
plausible distractors).
- Clearly indicate the correct answer.
The output should be in JSON format, with each
question as a dictionary containing:
- "question": The MCQ question.
- "options": A list of 4 options (e.g., ["A: ..", "B:
..", "C: ..", "D: .."]).
- "answer": The correct answer (e.g., "A").
Output Example:
[
{
"question": "What is the primary role of a catalyst in a chemical reaction?",
"options": [
"A: To make a thermodynamically unfavorable reaction proceed",
"B: To provide a lower energy pathway between reactants and products",
"C: To decrease the rate of a chemical reaction",
"D: To change the overall reaction itself"
],
"answer": "B"
}
]
Passage: {passage}
MCQ Answering Prompt:
Given the contexts or evidences: {contexts}
Here is a multiple-choice question:
Question: {question}
Options: A. {options_0} B. {options_1} C. {options_2} D. {options_3}
Please select the correct answer by choosing A, B, C,
or D. Respond with only the letter of your choice.

Figure 8: The prompts for generating and answering
MCQ questions for evaluating knowledge retention in
knowledge graph.

24

Question: When did the country the top-ranking Warsaw Pact operatives came from, despite it being
headquartered in the country where A Generation is set, agree to a unified Germany inside NATO?
Combined Command
of Pact Armed
Forces

[headquater]

Warsaw
[Participated]

the Combined Command of Pact
Armed Forces controlled the
assigned multi-national
forces, with headquarters in
Warsaw, Poland
A Generation

[Located in]

[Participated]
Poland

[set in]

Figure 9: Event Node (green) offers enriched context over triplets (blue); dotted line indicates missing edge

Question: Who won the Indy Car Race in the largest populated city of the state where the performer of
Mingus Three is from?
[Concept]

United State

State

[Concept]

Arizona

[is the capital]

Myth
[Concept]

[Concept]

Champion
[Concept]

Mario Andretti

Country

[Participated]

race

[Concept]

[Concept]
The final
career victory
for Indy legend
Mario Andretti
(1993)

City

[At the same time]

Phoenix

[Participated]
Phoenix has a
rich history of
open wheel races

Figure 10: Concept nodes (orange) provide alternate pathways to access information beyond entities and events.

25

